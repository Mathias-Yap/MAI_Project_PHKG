{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T13:33:45.495812Z",
     "start_time": "2025-06-06T13:33:42.997326Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathiasyap/.local/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "from pipeline import pipelines\n",
    "from pipeline.simple_llm_query_generator import SimpleLLMQueryGenerator\n",
    "from pipeline.query_engine_component import QueryExecutorStep\n",
    "from pipeline.context.context_constructor import context_constructor\n",
    "import os\n",
    "import traceback\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "def run_pipelines(natural_language_question: list, expected_sparql: list):\n",
    "    sparql_llm_pipeline = None\n",
    "    sparql_execution_pipeline = None\n",
    "    try:\n",
    "        query_generator = SimpleLLMQueryGenerator(model_name=\"meta-llama/LLama-3.2-1B-Instruct\")\n",
    "        query_executor = QueryExecutorStep(\n",
    "            engine_name=\"milleniumDB\",\n",
    "            graph_path=\"rdf_100_sphn.nt\",\n",
    "            verbose=True,\n",
    "            query_format=\"sparql\",\n",
    "            construct_graph=False\n",
    "        )\n",
    "        context_maker = context_constructor()\n",
    "        sparql_llm_pipeline = pipelines.InitialPipeline([query_generator, query_executor])\n",
    "        sparql_execution_pipeline = pipelines.InitialPipeline([query_executor])\n",
    "\n",
    "        sparql_llm_pipeline.initialize()\n",
    "        sparql_execution_pipeline.initialize()\n",
    "        results_dict = {\n",
    "            'sparql_without_context': [],\n",
    "            'sparql_with_context': [],\n",
    "            'expected_sparql_result': []\n",
    "        }\n",
    "        for question, sparql in notebook_tqdm(zip(natural_language_question, expected_sparql)):\n",
    "\n",
    "            context =  context_maker.context(question)\n",
    "            print(\"Context for the question:\", context)\n",
    "            result_llm_without_context = sparql_llm_pipeline.run(\n",
    "                natural_language_question=natural_language_question,\n",
    "                sparql_is_path=False\n",
    "            )\n",
    "            print(\"LLM Result without context:\", result_llm_without_context['query'])\n",
    "            result_llm_with_context = None\n",
    "            # Here context needs to be filled in depending on the Natural Language Question\n",
    "            result_llm_with_context = sparql_llm_pipeline.run(natural_language_question=natural_language_question,\n",
    "                                                              context=context,\n",
    "                                                              sparql_is_path=False)\n",
    "            print(\"LLM Result with context:\", result_llm_with_context['query'])\n",
    "            expected_sparql_result = sparql_execution_pipeline.run({\"query\": sparql})\n",
    "            results_dict['sparql_without_context'].append(result_llm_without_context['query'])\n",
    "            results_dict['sparql_with_context'].append(result_llm_with_context['query'])\n",
    "            results_dict['expected_sparql_result'].append(expected_sparql_result)\n",
    "\n",
    "        return results_dict\n",
    "\n",
    "    except Exception as ex:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        if sparql_llm_pipeline:\n",
    "            try:\n",
    "                sparql_llm_pipeline.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error while closing sparql_llm_pipeline: {e}\")\n",
    "        if sparql_execution_pipeline:\n",
    "            try:\n",
    "                sparql_execution_pipeline.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error while closing sparql_execution_pipeline: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d998a8a0c7e470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T13:33:45.544196Z",
     "start_time": "2025-06-06T13:33:45.498317Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "updated_q_sparql_pairs = pd.read_csv(\"../sparql_queries/updated_qq_pairs/question_query_pairs_100.csv\")\n",
    "# Get 100 random samples\n",
    "sampled_pairs = updated_q_sparql_pairs.sample(n=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2518d82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_pairs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138d98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = run_pipelines(\n",
    "#     natural_language_question=sampled_pairs['Question'].tolist(),\n",
    "#     expected_sparql=sampled_pairs['SPARQL Query'].tolist()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5abb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014fa0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Checking available images...\n",
      "['mdb:latest']\n",
      "['docker-aseprite:latest']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-transport']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-energy']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-neuro']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-cancer']\n",
      "['hello-world:latest']\n",
      "['ghcr.io/avantlab/avantgraph:ckg']\n",
      "['ghcr.io/avantlab/avantgraph:release-2024-01-31']\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from pipeline import mdb_validated_generation\n",
    "generate_execute_component = mdb_validated_generation.MDBValidatedGeneration(model_name = \"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "val_pipeline  = pipelines.InitialPipeline([generate_execute_component])\n",
    "context_maker = context_constructor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11d82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Next follow template questions and queries related to the natural language question:\\nQuestion: How many substances does {DRUG} contain?\\nQuery: SELECT COUNT(DISTINCT substance) FROM substance WHERE drug = {{DRUG}}\\n\\nThe following class IRIs can be placed in place of the classes enclosed by brackets \\\\{\\\\} in the templates:\\nClass IRI: ['https://www.biomedit.ch/rdf/sphn-schema/sphn/Substance', 'https://www.biomedit.ch/rdf/sphn-schema/sphn/Drug']\\nClass IRI: []\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_maker.get_context(question = \"how many substances does drug 12035 contain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa9a148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next follow template questions and queries related to the natural language question:\n",
      "Question: How many substances does {DRUG} contain?\n",
      "Query: SELECT COUNT(DISTINCT substance) FROM substance WHERE drug = {{DRUG}}\n",
      "\n",
      "The following class IRIs can be placed in place of the classes enclosed by brackets \\{\\} in the templates:\n",
      "Class IRI: ['https://www.biomedit.ch/rdf/sphn-schema/sphn/Substance', 'https://www.biomedit.ch/rdf/sphn-schema/sphn/Drug']\n",
      "Class IRI: []\n",
      "\n",
      "â–¶ Running step: MDBValidatedGeneration\n",
      "\n",
      "ðŸ§  Attempt 0: Executing query...\n",
      "\n",
      "        SELECT COUNT(DISTINCT substance) FROM substance WHERE drug = __DRUG__\n",
      "        \n",
      "\n",
      ">>> Query being sent to MillenniumDB:\n",
      " \n",
      "        SELECT COUNT(DISTINCT substance) FROM substance WHERE drug = __DRUG__\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query fixing prompt: This SPARQL query returned an error:  extraneous input 'COUNT' expecting {DISTINCT, REDUCED, VAR1, VAR2, '(', '*'}. Fix the query and try again. Again, do not include any explanations or apologies in your responses.\n",
      "\n",
      "âœ… Fixed SPARQL Query:\n",
      " PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "PREFIX obo: <http://purl.obolibrary.org/obo/>\n",
      "\n",
      "SELECT?substance\n",
      "WHERE {\n",
      "   ?drug obo:Drug?substance.\n",
      "    FILTER(relation?drug = obo:Drug)\n",
      "}\n",
      "\n",
      "ðŸ§  Attempt 1: Executing query...\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "PREFIX obo: <http://purl.obolibrary.org/obo/>\n",
      "\n",
      "SELECT?substance\n",
      "WHERE {\n",
      "   ?drug obo:Drug?substance.\n",
      "    FILTER(relation?drug = obo:Drug)\n",
      "}\n",
      "\n",
      ">>> Query being sent to MillenniumDB:\n",
      " PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "PREFIX obo: <http://purl.obolibrary.org/obo/>\n",
      "\n",
      "SELECT?substance\n",
      "WHERE {\n",
      "   ?drug obo:Drug?substance.\n",
      "    FILTER(relation?drug = obo:Drug)\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 0/1 [00:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMillenniumDBError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[31mMillenniumDBError\u001b[39m: Query Exception: mismatched input 'r' expecting {STR, LANG, LANGMATCHES, DATATYPE, BOUND, SAMETERM, ISIRI, ISURI, ISBLANK, ISLITERAL, REGEX, SUBSTR, TRUE, FALSE, IRI, URI, BNODE, RAND, ABS, CEIL, FLOOR, ROUND, CONCAT, STRLEN, UCASE, LCASE, ENCODE_FOR_URI, CONTAINS, STRSTARTS, STRENDS, STRBEFORE, STRAFTER, REPLACE, YEAR, MONTH, DAY, HOURS, MINUTES, SECONDS, TIMEZONE, TZ, NOW, UUID, STRUUID, MD5, SHA1, SHA256, SHA384, SHA512, COALESCE, IF, STRLANG, STRDT, ISNUMERIC, COUNT, SUM, MIN, MAX, AVG, SAMPLE, GROUP_CONCAT, NOT, EXISTS, IRIREF, PNAME_NS, PNAME_LN, VAR1, VAR2, INTEGER, DECIMAL, DOUBLE, INTEGER_POSITIVE, DECIMAL_POSITIVE, DOUBLE_POSITIVE, INTEGER_NEGATIVE, DECIMAL_NEGATIVE, DOUBLE_NEGATIVE, STRING_LITERAL1, STRING_LITERAL2, STRING_LITERAL_LONG1, STRING_LITERAL_LONG2, '(', '+', '-', '!'}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mResultError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/mdb_validated_generation.py:33\u001b[39m, in \u001b[36mMDBValidatedGeneration.run\u001b[39m\u001b[34m(self, data, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Execute the query using the query executor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/query_engine_component.py:47\u001b[39m, in \u001b[36mQueryExecutorStep.run\u001b[39m\u001b[34m(self, data, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     sparql_is_path = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m]=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43msparql_is_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/query_engine_component.py:96\u001b[39m, in \u001b[36mQueryExecutorStep.query\u001b[39m\u001b[34m(self, query, path)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmdb_server_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mavantgraph\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/query_engine_component.py:115\u001b[39m, in \u001b[36mQueryExecutorStep.mdb_server_query\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    114\u001b[39m session = driver.session()\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m result = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/session.py:21\u001b[39m, in \u001b[36m_ensure_session_open.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MillenniumDBError(\u001b[33m\"\u001b[39m\u001b[33mSession Error: session is closed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/session.py:44\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, query, timeout)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03mRun a query on the server.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m:return: The result of the query.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_message_receiver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_response_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/result.py:38\u001b[39m, in \u001b[36mResult.__init__\u001b[39m\u001b[34m(self, driver, connection, message_receiver, response_handler, query, timeout)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m._response_handler = response_handler\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/result.py:118\u001b[39m, in \u001b[36mResult._run\u001b[39m\u001b[34m(self, query, timeout)\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mself\u001b[39m._message_receiver.receive()\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_response_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# on_record / on_success\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/response_handler.py:29\u001b[39m, in \u001b[36mResponseHandler.handle\u001b[39m\u001b[34m(self, message)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m protocol.ResponseType.ERROR:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_error\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMillenniumDBError\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpayload\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m._next_observer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/response_handler.py:64\u001b[39m, in \u001b[36mResponseHandler._callback\u001b[39m\u001b[34m(self, callback_key, *args, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m._current_observer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m callback_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._current_observer\n\u001b[32m     63\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_current_observer\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcallback_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/result.py:106\u001b[39m, in \u001b[36mResult._run.<locals>.on_error\u001b[39m\u001b[34m(error)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m._exception = error\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ResultError(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mself\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exception\u001b[39;00m\n",
      "\u001b[31mResultError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m result_llm_with_context = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Here context needs to be filled in depending on the Natural Language Question\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result_llm_with_context = \u001b[43mval_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnatural_language_question\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnatural_language_question\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                                  \u001b[49m\u001b[43msparql_is_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLM Result with context:\u001b[39m\u001b[33m\"\u001b[39m, result_llm_with_context[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/pipelines.py:24\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, initial_data, **kwargs)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâ–¶ Running step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m data = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m end_time = time.time()\n\u001b[32m     26\u001b[39m duration = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/mdb_validated_generation.py:44\u001b[39m, in \u001b[36mMDBValidatedGeneration.run\u001b[39m\u001b[34m(self, data, **kwargs)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâŒ Maximum attempts reached. Unable to generate a valid query.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll attempts to fix the SPARQL query failed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m query = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_query_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/simple_llm_query_generator.py:180\u001b[39m, in \u001b[36mSimpleLLMQueryGenerator.handle_query_error\u001b[39m\u001b[34m(self, query, error, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m system_prompt = (\n\u001b[32m    169\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou are a system that returns only raw SPARQL queries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDo NOT include explanations, JSON, or any other formatting. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    171\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mReturn only the SPARQL query, optionally inside triple backticks.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    172\u001b[39m )\n\u001b[32m    173\u001b[39m prompt = (\n\u001b[32m    174\u001b[39m     VALIDATION_PROMPT.format(\n\u001b[32m    175\u001b[39m     ontology=\u001b[38;5;28mself\u001b[39m.vocabulary,\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m     )\n\u001b[32m    179\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m raw_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_model_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuery fixing prompt: \u001b[39m\u001b[33m\"\u001b[39m + error_message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/llm/llm_pipeline.py:25\u001b[39m, in \u001b[36mModelPipeline.generate\u001b[39m\u001b[34m(self, prompt, system_prompt)\u001b[39m\n\u001b[32m     18\u001b[39m chat.add_user_message(prompt)\n\u001b[32m     19\u001b[39m pipeline = transformers.pipeline(\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m     22\u001b[39m     torch_dtype=torch.bfloat16,\n\u001b[32m     23\u001b[39m     device=\u001b[32m0\u001b[39m,\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m result = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:295\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    297\u001b[39m         chats = (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# ðŸˆ ðŸˆ ðŸˆ\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/base.py:1431\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1424\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1425\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1428\u001b[39m         )\n\u001b[32m   1429\u001b[39m     )\n\u001b[32m   1430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/base.py:1438\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1437\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1439\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/base.py:1338\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1337\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:400\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    398\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    403\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/generation/utils.py:3560\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3562\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3563\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3564\u001b[39m     outputs,\n\u001b[32m   3565\u001b[39m     model_kwargs,\n\u001b[32m   3566\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m output_hidden_states = (\n\u001b[32m    684\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    701\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    451\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:324\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    322\u001b[39m residual = hidden_states\n\u001b[32m    323\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    327\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:162\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "natural_language_question = [\"How many substances does drug 12305 contain?\"]\n",
    "for question in notebook_tqdm((natural_language_question)):\n",
    "    context =  context_maker.get_context(question = question)\n",
    "    print(context)\n",
    "    result_llm_with_context = None\n",
    "    # Here context needs to be filled in depending on the Natural Language Question\n",
    "    result_llm_with_context = val_pipeline.run(natural_language_question=natural_language_question,\n",
    "                                                    context = context,\n",
    "                                                      sparql_is_path=False)\n",
    "    print(\"LLM Result with context:\", result_llm_with_context['query'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4607323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathiasyap/.local/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "from pipeline import mdb_validated_generation\n",
    "from pipeline.context.context_constructor import context_constructor\n",
    "def run_validated_pipeline(natural_language_question):\n",
    "    generate_execute_component = mdb_validated_generation.MDBValidatedGeneration(model_name = \"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "    val_pipeline  = pipelines.InitialPipeline([generate_execute_component])\n",
    "    context_maker = context_constructor(construct_vector_store=True, example_queries_file=\"/home/mathiasyap/Code/university/phkg/MAI_Project_PHKG/src/pipeline/context/qq_pairs.yaml\")\n",
    "    val_pipeline.initialize()\n",
    "    try: \n",
    "        for question in notebook_tqdm((natural_language_question)):\n",
    "            context =  context_maker.get_context(question)\n",
    "            print(context)\n",
    "            result_llm_with_context = None\n",
    "            # Here context needs to be filled in depending on the Natural Language Question\n",
    "            result_llm_with_context = val_pipeline.run(natural_language_question=natural_language_question,\n",
    "                                                              sparql_is_path=False)\n",
    "            print(\"LLM Result with context:\", result_llm_with_context['query'])\n",
    "\n",
    "        return result_llm_with_context\n",
    "\n",
    "    except Exception as ex:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            val_pipeline.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error while closing pipeline: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca68f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "['mdb:latest']\n",
      "['docker-aseprite:latest']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-transport']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-energy']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-neuro']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-cancer']\n",
      "['hello-world:latest']\n",
      "['ghcr.io/avantlab/avantgraph:ckg']\n",
      "['ghcr.io/avantlab/avantgraph:release-2024-01-31']\n",
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pipelines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data_try = \u001b[43mrun_validated_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnatural_language_question\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHow many substances does drug 12305 contain?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mrun_validated_pipeline\u001b[39m\u001b[34m(natural_language_question)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_validated_pipeline\u001b[39m(natural_language_question):\n\u001b[32m      4\u001b[39m     generate_execute_component = mdb_validated_generation.MDBValidatedGeneration(model_name = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-1B-Instruct\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     val_pipeline  = \u001b[43mpipelines\u001b[49m.InitialPipeline([generate_execute_component])\n\u001b[32m      6\u001b[39m     context_maker = context_constructor(construct_vector_store=\u001b[38;5;28;01mTrue\u001b[39;00m, example_queries_file=\u001b[33m\"\u001b[39m\u001b[33m/home/mathiasyap/Code/university/phkg/MAI_Project_PHKG/src/pipeline/context/qq_pairs.yaml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     val_pipeline.initialize()\n",
      "\u001b[31mNameError\u001b[39m: name 'pipelines' is not defined"
     ]
    }
   ],
   "source": [
    "data_try = run_validated_pipeline(\n",
    "    natural_language_question=\"How many substances does drug 12305 contain?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_query_engine():\n",
    "    executor = QueryExecutorStep(\n",
    "        engine_name=\"milleniumDB\",\n",
    "        graph_path=\"rdf_100_sphn.nt\",\n",
    "        verbose=True,\n",
    "        query_format=\"sparql\",\n",
    "        construct_graph=False\n",
    "    )\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa3a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking available images...\n",
      "['mdb:latest']\n",
      "['docker-aseprite:latest']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-transport']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-energy']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-neuro']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-cancer']\n",
      "['hello-world:latest']\n",
      "['ghcr.io/avantlab/avantgraph:ckg']\n",
      "['ghcr.io/avantlab/avantgraph:release-2024-01-31']\n"
     ]
    }
   ],
   "source": [
    "engine = initialize_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ecd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Query being sent to MillenniumDB:\n",
      " Select ?1 .... {}\n"
     ]
    },
    {
     "ename": "ResultError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMillenniumDBError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[31mMillenniumDBError\u001b[39m: Query Exception: mismatched input '.' expecting {FROM, WHERE, '{'}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mResultError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSelect ?1 .... \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/query_engine_component.py:96\u001b[39m, in \u001b[36mQueryExecutorStep.query\u001b[39m\u001b[34m(self, query, path)\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mdb_server_query_path(query)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmdb_server_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mavantgraph\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m path:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/query_engine_component.py:115\u001b[39m, in \u001b[36mQueryExecutorStep.mdb_server_query\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    113\u001b[39m driver = millenniumdb_driver.driver(url)\n\u001b[32m    114\u001b[39m session = driver.session()\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m result = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/session.py:21\u001b[39m, in \u001b[36m_ensure_session_open.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._open:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MillenniumDBError(\u001b[33m\"\u001b[39m\u001b[33mSession Error: session is closed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/session.py:44\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, query, timeout)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@_ensure_session_open\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.0\u001b[39m) -> Result:\n\u001b[32m     40\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    Run a query on the server.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    :return: The result of the query.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_driver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_message_receiver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_response_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/result.py:38\u001b[39m, in \u001b[36mResult.__init__\u001b[39m\u001b[34m(self, driver, connection, message_receiver, response_handler, query, timeout)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m._message_receiver = message_receiver\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m._response_handler = response_handler\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/result.py:118\u001b[39m, in \u001b[36mResult._run\u001b[39m\u001b[34m(self, query, timeout)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# on_variables\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mself\u001b[39m._message_receiver.receive()\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_response_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# on_record / on_success\u001b[39;00m\n\u001b[32m    121\u001b[39m raw_records, termination_message = \u001b[38;5;28mself\u001b[39m._message_receiver.receive_records()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/response_handler.py:29\u001b[39m, in \u001b[36mResponseHandler.handle\u001b[39m\u001b[34m(self, message)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mself\u001b[39m._next_observer()\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m protocol.ResponseType.ERROR:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_error\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMillenniumDBError\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpayload\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m._next_observer()\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m protocol.ResponseType.VARIABLES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/response_handler.py:64\u001b[39m, in \u001b[36mResponseHandler._callback\u001b[39m\u001b[34m(self, callback_key, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03mCall the observer with the given key and arguments.\u001b[39;00m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[33;03m:param callback_key: The key of the observer to call.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m:type callback_key: str\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m._current_observer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m callback_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._current_observer\n\u001b[32m     63\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_current_observer\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcallback_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/millenniumdb_driver/result.py:106\u001b[39m, in \u001b[36mResult._run.<locals>.on_error\u001b[39m\u001b[34m(error)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m._streaming = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m._exception = error\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ResultError(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mself\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exception\u001b[39;00m\n",
      "\u001b[31mResultError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "engine.query(\"Select ?1 .... {}\", path = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c1963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run pipeline for question: How many patients were diagnosed with diagnosis 102358?\n",
      "Using device: cpu\n",
      "Checking available images...\n",
      "['mdb:latest']\n",
      "['docker-aseprite:latest']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-transport']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-energy']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-neuro']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-cancer']\n",
      "['hello-world:latest']\n",
      "['ghcr.io/avantlab/avantgraph:ckg']\n",
      "['ghcr.io/avantlab/avantgraph:release-2024-01-31']\n",
      "â–¶ Running step: SimpleLLMQueryGenerator\n",
      "Prompt: \n",
      "Task: Generate a SPARQL SELECT statement for querying a graph database.\n",
      "For instance, to find all email addresses of John Doe, the following query in backticks would be suitable:\n",
      "```\n",
      "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
      "SELECT ?email\n",
      "WHERE {\n",
      "    ?person foaf:name \"John Doe\" .\n",
      "    ?person foaf:mbox ?email .\n",
      "}\n",
      "```\n",
      "Keep in mind that you might need several classes in order to provide the correct answer. \n",
      "\n",
      "Instructions:\n",
      "Use only the node types and properties provided in the ontology.\n",
      "Do not use any node types and properties that are not explicitly provided.\n",
      "Include all necessary prefixes and relations.\n",
      "\n",
      "The ontology is:\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix sphn: <https://www.biomedit.ch/rdf/sphn-schema/sphn/> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "sphn:hasActiveIngredient a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Active Ingredient\" ;\n",
      "    rdfs:comment \"Active component of the concept.\" ;\n",
      "    rdfs:domain sphn:Drug ;\n",
      "    rdfs:range sphn:Substance .\n",
      "\n",
      "sphn:hasCode a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Code\" ;\n",
      "    rdfs:comment \"Coded information specifying the concept.\" ;\n",
      "    rdfs:domain sphn:Diagnosis,\n",
      "        sphn:LabResult,\n",
      "        sphn:MedicalProcedure,\n",
      "        sphn:Substance ;\n",
      "    rdfs:range sphn:Code .\n",
      "\n",
      "sphn:hasCodingSystemAndVersion a owl:DatatypeProperty ;\n",
      "    rdfs:label \"has Coding System and Version\" ;\n",
      "    rdfs:comment \"Name and version of the coding system.\" ;\n",
      "    rdfs:domain sphn:Code ;\n",
      "    rdfs:range xsd:string .\n",
      "\n",
      "sphn:hasDrug a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Drug\" ;\n",
      "    rdfs:comment \"Medication associated to the concept.\" ;\n",
      "    rdfs:domain sphn:DrugPrescription ;\n",
      "    rdfs:range sphn:Drug .\n",
      "\n",
      "sphn:hasLabTest a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Lab Test\" ;\n",
      "    rdfs:comment \"Lab test associated to the concept.\" ;\n",
      "    rdfs:domain sphn:LabTestEvent ;\n",
      "    rdfs:range sphn:LabResult .\n",
      "\n",
      "sphn:hasSubjectPseudoIdentifier a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Subject Pseudo Identifier\" ;\n",
      "    rdfs:domain sphn:Diagnosis,\n",
      "        sphn:DrugPrescription,\n",
      "        sphn:LabTestEvent,\n",
      "        sphn:MedicalProcedure ;\n",
      "    rdfs:range sphn:SubjectPseudoIdentifier .\n",
      "\n",
      "sphn:SubjectPseudoIdentifier a owl:Class ;\n",
      "    rdfs:label \"Patient Identifier\" ;\n",
      "    rdfs:comment \"A coded unique identifier assigned by a data provider for a specific project to conceal the identity of an individual.\" .\n",
      "\n",
      "sphn:Code a owl:Class ;\n",
      "    rdfs:label \"Code\" ;\n",
      "    rdfs:comment \"Symbols and/or expressions defined in a coding system.\" .\n",
      "\n",
      "sphn:Diagnosis a owl:Class ;\n",
      "    rdfs:label \"Diagnosis\" ;\n",
      "    rdfs:comment \"Determination of the presence of a disease, condition, or injury from expressed signs and symptoms and assessments such as physical examination, laboratory test, or the like.\" .\n",
      "\n",
      "sphn:Drug a owl:Class ;\n",
      "    rdfs:label \"Drug\" ;\n",
      "    rdfs:comment \"Any substance with the intent to prevent, diagnose, treat, or relieve symptoms of a disease or abnormal condition.\" .\n",
      "\n",
      "sphn:DrugPrescription a owl:Class ;\n",
      "    rdfs:label \"Drug Prescription\" ;\n",
      "    rdfs:comment \"Plan that defines at which frequency a drug should be administered to a patient with a given quantity; at every frequency time point a drug administration event should occur.\" .\n",
      "\n",
      "sphn:LabResult a owl:Class ;\n",
      "    rdfs:label \"Lab Result\" ;\n",
      "    rdfs:comment \"Outcome, value, or information which gives insight about a laboratory test.\" .\n",
      "\n",
      "sphn:LabTestEvent a owl:Class ;\n",
      "    rdfs:label \"Lab Test Event\" ;\n",
      "    rdfs:comment \"Occurrence in which one or multiple laboratory tests are performed on a biological specimen at a given time.\" .\n",
      "\n",
      "sphn:MedicalProcedure a owl:Class ;\n",
      "    rdfs:label \"Medical Procedure\" ;\n",
      "    rdfs:comment \"Invasive or non-invasive intervention performed for, with or on behalf of an individual whose purpose is to assess, improve, maintain, promote or modify health, functioning or health conditions.\" .\n",
      "\n",
      "sphn:Substance a owl:Class ;\n",
      "    rdfs:label \"Substance\" ;\n",
      "    rdfs:comment \"Any matter of defined composition that has discrete existence, whose origin may be biological, mineral or chemical.\" .\n",
      "\n",
      "\n",
      "\n",
      "Note: Be as concise as possible.\n",
      "Do not include any explanations or apologies in your responses.\n",
      "Do not respond to any questions that ask for anything else than for you to construct a SPARQL query.\n",
      "Do not include any text except for the SPARQL query generated.\n",
      "\n",
      "The question is:\n",
      "How many patients were diagnosed with diagnosis 102358?\n",
      "\n",
      "Here is some additional content you might want to consider:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ðŸ§  Attempt 1: Prompting LLM...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 30.48it/s]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Raw LLM Output:\n",
      " ```sparql\n",
      "PREFIX sphn: <https://www.biomedit.ch/rdf/sphn-schema/sphn/>\n",
      "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "SELECT COUNT (?patient)\n",
      "WHERE {\n",
      "   ?patient sphn:hasSubjectPseudoIdentifier?identifier.\n",
      "   ?identifier sphn:hasCodingSystemAndVersion?coding.\n",
      "   ?coding sphn:hasCode?code.\n",
      "   ?code sphn:hasActiveIngredient?activeIngredient.\n",
      "   ?activeIngredient a sphn:Drug.\n",
      "   ?patient sphn:hasDiagnosis?diagnosis.\n",
      "   ?diagnosis sphn:id \"102358\".\n",
      "}\n",
      "```\n",
      "\n",
      "âœ… Final SPARQL Query:\n",
      " PREFIX sphn: <https://www.biomedit.ch/rdf/sphn-schema/sphn/>\n",
      "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "SELECT COUNT (?patient)\n",
      "WHERE {\n",
      "   ?patient sphn:hasSubjectPseudoIdentifier?identifier.\n",
      "   ?identifier sphn:hasCodingSystemAndVersion?coding.\n",
      "   ?coding sphn:hasCode?code.\n",
      "   ?code sphn:hasActiveIngredient?activeIngredient.\n",
      "   ?activeIngredient a sphn:Drug.\n",
      "   ?patient sphn:hasDiagnosis?diagnosis.\n",
      "   ?diagnosis sphn:id \"102358\".\n",
      "}\n",
      "âœ… Step 'SimpleLLMQueryGenerator' completed in 66.07 seconds\n",
      "\n",
      "â–¶ Running step: QueryExecutorStep\n",
      ">>> Query being sent to MillenniumDB:\n",
      " PREFIX sphn: <https://www.biomedit.ch/rdf/sphn-schema/sphn/>\n",
      "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "SELECT COUNT (?patient)\n",
      "WHERE {\n",
      "   ?patient sphn:hasSubjectPseudoIdentifier?identifier.\n",
      "   ?identifier sphn:hasCodingSystemAndVersion?coding.\n",
      "   ?coding sphn:hasCode?code.\n",
      "   ?code sphn:hasActiveIngredient?activeIngredient.\n",
      "   ?activeIngredient a sphn:Drug.\n",
      "   ?patient sphn:hasDiagnosis?diagnosis.\n",
      "   ?diagnosis sphn:id \"102358\".\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "millenniumdb_driver.millenniumdb_error.MillenniumDBError: Query Exception: extraneous input 'COUNT' expecting {DISTINCT, REDUCED, VAR1, VAR2, '(', '*'}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_314576/45117174.py\", line 27, in run_pipelines\n",
      "    result_llm_without_context = sparql_llm_pipeline.run(\n",
      "        natural_language_question=natural_language_question,\n",
      "        sparql_is_path=False\n",
      "    )\n",
      "  File \"/home/mathiasyap/Code/university/phkg/MAI_Project_PHKG/src/pipeline/pipelines.py\", line 24, in run\n",
      "    data = step.run(data, **kwargs)\n",
      "  File \"/home/mathiasyap/Code/university/phkg/MAI_Project_PHKG/src/pipeline/query_engine_component.py\", line 47, in run\n",
      "    data[\"result\"]=self.query(query, path=sparql_is_path)\n",
      "                   ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mathiasyap/Code/university/phkg/MAI_Project_PHKG/src/pipeline/query_engine_component.py\", line 96, in query\n",
      "    return self.mdb_server_query(query)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/home/mathiasyap/Code/university/phkg/MAI_Project_PHKG/src/pipeline/query_engine_component.py\", line 115, in mdb_server_query\n",
      "    result = session.run(query)\n",
      "  File \"/home/mathiasyap/.local/lib/python3.13/site-packages/millenniumdb_driver/session.py\", line 21, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/mathiasyap/.local/lib/python3.13/site-packages/millenniumdb_driver/session.py\", line 44, in run\n",
      "    return Result(\n",
      "        self._driver,\n",
      "    ...<4 lines>...\n",
      "        timeout,\n",
      "    )\n",
      "  File \"/home/mathiasyap/.local/lib/python3.13/site-packages/millenniumdb_driver/result.py\", line 38, in __init__\n",
      "    self._run(query, timeout)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mathiasyap/.local/lib/python3.13/site-packages/millenniumdb_driver/result.py\", line 118, in _run\n",
      "    self._response_handler.handle(message)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
      "  File \"/home/mathiasyap/.local/lib/python3.13/site-packages/millenniumdb_driver/response_handler.py\", line 29, in handle\n",
      "    self._callback(\"on_error\", MillenniumDBError(message[\"payload\"]))\n",
      "    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mathiasyap/.local/lib/python3.13/site-packages/millenniumdb_driver/response_handler.py\", line 64, in _callback\n",
      "    self._current_observer[callback_key](*args, **kwargs)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mathiasyap/.local/lib/python3.13/site-packages/millenniumdb_driver/result.py\", line 106, in on_error\n",
      "    raise ResultError(self) from self._exception\n",
      "millenniumdb_driver.millenniumdb_error.ResultError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while closing sparql_execution_pipeline: 404 Client Error for http+docker://localhost/v1.49/containers/b179469f6b5f239c6c3d2c91fb44b7651d71a5233e2e0afb814e41a5427f2a5f?v=True&link=False&force=True: Not Found (\"No such container: b179469f6b5f239c6c3d2c91fb44b7651d71a5233e2e0afb814e41a5427f2a5f\")\n"
     ]
    }
   ],
   "source": [
    "question = \"How many patients were diagnosed with diagnosis 102358?\"\n",
    "expected_sparql = \"\"\"\n",
    "SELECT (COUNT(?patient) AS ?count)\n",
    "WHERE {\n",
    "  ?patient a <http://www.semanticweb.org/ontologies/2023/10/untitled-ontology-2#Patient> .\n",
    "  ?patient <http://www.semanticweb.org/ontologies/2023/10/untitled-ontology-2#hasDiagnosis> <http://www.semanticweb.org/ontologies/2023/10/untitled-ontology-2#Diagnosis102358> .\n",
    "}\n",
    "\"\"\"\n",
    "result_llm_without_context_list = []\n",
    "result_llm_with_context_list = []\n",
    "expected_sparql_result_list = []\n",
    "print(f\"Run pipeline for question: {question}\")\n",
    "result_llm_without_context, result_llm_with_context, expected_sparql_result = run_pipelines(\n",
    "    natural_language_question=question,\n",
    "    expected_sparql=expected_sparql)\n",
    "if result_llm_without_context:\n",
    "    result_llm_without_context_list.append(result_llm_without_context[\"result\"])\n",
    "else:\n",
    "    result_llm_without_context_list.append(None)\n",
    "\n",
    "if result_llm_with_context:\n",
    "    result_llm_with_context_list.append(result_llm_with_context[\"result\"])\n",
    "else:\n",
    "    result_llm_with_context_list.append(None)\n",
    "\n",
    "if expected_sparql_result:\n",
    "    expected_sparql_result_list.append(expected_sparql_result[\"result\"])\n",
    "else:\n",
    "    expected_sparql_result_list.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3bdfa2774284ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T13:35:37.459235Z",
     "start_time": "2025-06-06T13:33:45.673133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run pipeline for question: What are the codes for the substances used in the drug prescription 17205470?\n",
      "Using device: cpu\n",
      "Checking available images...\n",
      "['mdb:latest']\n",
      "['docker-aseprite:latest']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-transport']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-energy']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-neuro']\n",
      "['ghcr.io/avantlab/avantgraph:openaire-cancer']\n",
      "['hello-world:latest']\n",
      "['ghcr.io/avantlab/avantgraph:ckg']\n",
      "['ghcr.io/avantlab/avantgraph:release-2024-01-31']\n",
      "â–¶ Running step: SimpleLLMQueryGenerator\n",
      "Prompt: \n",
      "Task: Generate a SPARQL SELECT statement for querying a graph database.\n",
      "For instance, to find all email addresses of John Doe, the following query in backticks would be suitable:\n",
      "```\n",
      "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
      "SELECT ?email\n",
      "WHERE {\n",
      "    ?person foaf:name \"John Doe\" .\n",
      "    ?person foaf:mbox ?email .\n",
      "}\n",
      "```\n",
      "Keep in mind that you might need several classes in order to provide the correct answer. \n",
      "\n",
      "Instructions:\n",
      "Use only the node types and properties provided in the ontology.\n",
      "Do not use any node types and properties that are not explicitly provided.\n",
      "Include all necessary prefixes and relations.\n",
      "\n",
      "The ontology is:\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix sphn: <https://www.biomedit.ch/rdf/sphn-schema/sphn/> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "sphn:hasActiveIngredient a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Active Ingredient\" ;\n",
      "    rdfs:comment \"Active component of the concept.\" ;\n",
      "    rdfs:domain sphn:Drug ;\n",
      "    rdfs:range sphn:Substance .\n",
      "\n",
      "sphn:hasCode a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Code\" ;\n",
      "    rdfs:comment \"Coded information specifying the concept.\" ;\n",
      "    rdfs:domain sphn:Diagnosis,\n",
      "        sphn:LabResult,\n",
      "        sphn:MedicalProcedure,\n",
      "        sphn:Substance ;\n",
      "    rdfs:range sphn:Code .\n",
      "\n",
      "sphn:hasCodingSystemAndVersion a owl:DatatypeProperty ;\n",
      "    rdfs:label \"has Coding System and Version\" ;\n",
      "    rdfs:comment \"Name and version of the coding system.\" ;\n",
      "    rdfs:domain sphn:Code ;\n",
      "    rdfs:range xsd:string .\n",
      "\n",
      "sphn:hasDrug a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Drug\" ;\n",
      "    rdfs:comment \"Medication associated to the concept.\" ;\n",
      "    rdfs:domain sphn:DrugPrescription ;\n",
      "    rdfs:range sphn:Drug .\n",
      "\n",
      "sphn:hasLabTest a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Lab Test\" ;\n",
      "    rdfs:comment \"Lab test associated to the concept.\" ;\n",
      "    rdfs:domain sphn:LabTestEvent ;\n",
      "    rdfs:range sphn:LabResult .\n",
      "\n",
      "sphn:hasSubjectPseudoIdentifier a owl:ObjectProperty ;\n",
      "    rdfs:label \"has Subject Pseudo Identifier\" ;\n",
      "    rdfs:domain sphn:Diagnosis,\n",
      "        sphn:DrugPrescription,\n",
      "        sphn:LabTestEvent,\n",
      "        sphn:MedicalProcedure ;\n",
      "    rdfs:range sphn:SubjectPseudoIdentifier .\n",
      "\n",
      "sphn:SubjectPseudoIdentifier a owl:Class ;\n",
      "    rdfs:label \"Patient Identifier\" ;\n",
      "    rdfs:comment \"A coded unique identifier assigned by a data provider for a specific project to conceal the identity of an individual.\" .\n",
      "\n",
      "sphn:Code a owl:Class ;\n",
      "    rdfs:label \"Code\" ;\n",
      "    rdfs:comment \"Symbols and/or expressions defined in a coding system.\" .\n",
      "\n",
      "sphn:Diagnosis a owl:Class ;\n",
      "    rdfs:label \"Diagnosis\" ;\n",
      "    rdfs:comment \"Determination of the presence of a disease, condition, or injury from expressed signs and symptoms and assessments such as physical examination, laboratory test, or the like.\" .\n",
      "\n",
      "sphn:Drug a owl:Class ;\n",
      "    rdfs:label \"Drug\" ;\n",
      "    rdfs:comment \"Any substance with the intent to prevent, diagnose, treat, or relieve symptoms of a disease or abnormal condition.\" .\n",
      "\n",
      "sphn:DrugPrescription a owl:Class ;\n",
      "    rdfs:label \"Drug Prescription\" ;\n",
      "    rdfs:comment \"Plan that defines at which frequency a drug should be administered to a patient with a given quantity; at every frequency time point a drug administration event should occur.\" .\n",
      "\n",
      "sphn:LabResult a owl:Class ;\n",
      "    rdfs:label \"Lab Result\" ;\n",
      "    rdfs:comment \"Outcome, value, or information which gives insight about a laboratory test.\" .\n",
      "\n",
      "sphn:LabTestEvent a owl:Class ;\n",
      "    rdfs:label \"Lab Test Event\" ;\n",
      "    rdfs:comment \"Occurrence in which one or multiple laboratory tests are performed on a biological specimen at a given time.\" .\n",
      "\n",
      "sphn:MedicalProcedure a owl:Class ;\n",
      "    rdfs:label \"Medical Procedure\" ;\n",
      "    rdfs:comment \"Invasive or non-invasive intervention performed for, with or on behalf of an individual whose purpose is to assess, improve, maintain, promote or modify health, functioning or health conditions.\" .\n",
      "\n",
      "sphn:Substance a owl:Class ;\n",
      "    rdfs:label \"Substance\" ;\n",
      "    rdfs:comment \"Any matter of defined composition that has discrete existence, whose origin may be biological, mineral or chemical.\" .\n",
      "\n",
      "\n",
      "\n",
      "Note: Be as concise as possible.\n",
      "Do not include any explanations or apologies in your responses.\n",
      "Do not respond to any questions that ask for anything else than for you to construct a SPARQL query.\n",
      "Do not include any text except for the SPARQL query generated.\n",
      "\n",
      "The question is:\n",
      "What are the codes for the substances used in the drug prescription 17205470?\n",
      "\n",
      "Here is some additional content you might want to consider:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ðŸ§  Attempt 1: Prompting LLM...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 83.85it/s]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while closing sparql_execution_pipeline: 404 Client Error for http+docker://localhost/v1.49/containers/0d73a27ee79dd2db601a2b74980e1690e3b5183161704a6bb2f9bda1fd720b5c?v=True&link=False&force=True: Not Found (\"No such container: 0d73a27ee79dd2db601a2b74980e1690e3b5183161704a6bb2f9bda1fd720b5c\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m expected_sparql, question \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(queries, questions):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRun pipeline for question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     result_llm_without_context, result_llm_with_context, expected_sparql_result = \u001b[43mrun_pipelines\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnatural_language_question\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_sparql\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_sparql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result_llm_without_context:\n\u001b[32m     12\u001b[39m         result_llm_without_context_list.append(result_llm_without_context[\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mrun_pipelines\u001b[39m\u001b[34m(natural_language_question, expected_sparql)\u001b[39m\n\u001b[32m     24\u001b[39m sparql_llm_pipeline.initialize()\n\u001b[32m     25\u001b[39m sparql_execution_pipeline.initialize()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m result_llm_without_context = \u001b[43msparql_llm_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnatural_language_question\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnatural_language_question\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43msparql_is_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m result_llm_with_context = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Here context needs to be filled in depending on the Natural Language Question\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# result_llm_with_context = sparql_llm_pipeline.run(natural_language_question=natural_language_question,\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m#                                                   context=\"\",\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#                                                   sparql_is_path=False)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/pipelines.py:24\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, initial_data, **kwargs)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâ–¶ Running step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m data = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m end_time = time.time()\n\u001b[32m     26\u001b[39m duration = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/simple_llm_query_generator.py:129\u001b[39m, in \u001b[36mSimpleLLMQueryGenerator.run\u001b[39m\u001b[34m(self, data, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_retries + \u001b[32m1\u001b[39m):\n\u001b[32m    128\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ§  Attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Prompting LLM...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     raw_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_model_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“¥ Raw LLM Output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, raw_output)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/university/phkg/MAI_Project_PHKG/src/pipeline/llm/llm_pipeline.py:25\u001b[39m, in \u001b[36mModelPipeline.generate\u001b[39m\u001b[34m(self, prompt, system_prompt)\u001b[39m\n\u001b[32m     18\u001b[39m chat.add_user_message(prompt)\n\u001b[32m     19\u001b[39m pipeline = transformers.pipeline(\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m     22\u001b[39m     torch_dtype=torch.bfloat16,\n\u001b[32m     23\u001b[39m     device=\u001b[32m0\u001b[39m,\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m result = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:295\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    297\u001b[39m         chats = (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# ðŸˆ ðŸˆ ðŸˆ\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/base.py:1431\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1424\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1425\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1428\u001b[39m         )\n\u001b[32m   1429\u001b[39m     )\n\u001b[32m   1430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/base.py:1438\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1437\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1439\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/base.py:1338\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1337\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pipelines/text_generation.py:400\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    398\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    403\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/generation/utils.py:3557\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3554\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3557\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m output_hidden_states = (\n\u001b[32m    684\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    701\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    451\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    305\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:265\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    263\u001b[39m         attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    277\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py:54\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.jit.is_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch.Tensor):\n\u001b[32m     52\u001b[39m     is_causal = is_causal.item()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "queries = sampled_pairs[\"SPARQL Query\"].values\n",
    "questions = sampled_pairs[\"Question\"].values\n",
    "result_llm_without_context_list = []\n",
    "result_llm_with_context_list = []\n",
    "expected_sparql_result_list = []\n",
    "for expected_sparql, question in zip(queries, questions):\n",
    "    print(f\"Run pipeline for question: {question}\")\n",
    "    result_llm_without_context, result_llm_with_context, expected_sparql_result = run_pipelines(\n",
    "        natural_language_question=question,\n",
    "        expected_sparql=expected_sparql)\n",
    "    if result_llm_without_context:\n",
    "        result_llm_without_context_list.append(result_llm_without_context[\"result\"])\n",
    "    else:\n",
    "        result_llm_without_context_list.append(None)\n",
    "\n",
    "    if result_llm_with_context:\n",
    "        result_llm_with_context_list.append(result_llm_with_context[\"result\"])\n",
    "    else:\n",
    "        result_llm_with_context_list.append(None)\n",
    "\n",
    "    if expected_sparql_result:\n",
    "        expected_sparql_result_list.append(expected_sparql_result[\"result\"])\n",
    "    else:\n",
    "        expected_sparql_result_list.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667d4daa9bbf50a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T14:01:38.449817Z",
     "start_time": "2025-06-06T14:01:38.430508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Without Context ==\n",
      "Precision: 0.00\n",
      "Recall:    0.00\n",
      "Accuracy:  0.00\n",
      "\n",
      "== With Context ==\n",
      "Precision: 0.00\n",
      "Recall:    0.00\n",
      "Accuracy:  0.00\n"
     ]
    }
   ],
   "source": [
    "tp_with_context = fp_with_context = fn_with_context = 0\n",
    "tp_without_context = fp_without_context = fn_without_context = 0\n",
    "\n",
    "for result_llm_without_context, result_llm_with_context, result_exact in zip(\n",
    "        result_llm_without_context_list,\n",
    "        result_llm_with_context_list,\n",
    "        expected_sparql_result_list):\n",
    "\n",
    "    # Skip only if the expected result is missing\n",
    "    if not result_exact:\n",
    "        continue\n",
    "\n",
    "    values_exact = set(v[0] for v in result_exact.values())\n",
    "\n",
    "    # Without context\n",
    "    if result_llm_without_context:\n",
    "        values_llm_without = set(v[0] for v in result_llm_without_context.values())\n",
    "    else:\n",
    "        values_llm_without = set()\n",
    "\n",
    "    tp = len(values_llm_without & values_exact)\n",
    "    fp = len(values_llm_without - values_exact)\n",
    "    fn = len(values_exact - values_llm_without)\n",
    "    tp_without_context += tp\n",
    "    fp_without_context += fp\n",
    "    fn_without_context += fn\n",
    "\n",
    "    # With context\n",
    "    if result_llm_with_context:\n",
    "        values_llm_with = set(v[0] for v in result_llm_with_context.values())\n",
    "    else:\n",
    "        values_llm_with = set()\n",
    "\n",
    "    tp = len(values_llm_with & values_exact)\n",
    "    fp = len(values_llm_with - values_exact)\n",
    "    fn = len(values_exact - values_llm_with)\n",
    "    tp_with_context += tp\n",
    "    fp_with_context += fp\n",
    "    fn_with_context += fn\n",
    "\n",
    "# Metric functions\n",
    "def precision(tp, fp):\n",
    "    return tp / (tp + fp) if (tp + fp) else 0\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn) if (tp + fn) else 0\n",
    "\n",
    "def accuracy(tp, fp, fn):\n",
    "    total = tp + fp + fn\n",
    "    return tp / total if total else 0\n",
    "\n",
    "# Output\n",
    "print(\"\\n== Without Context ==\")\n",
    "print(f\"Precision: {precision(tp_without_context, fp_without_context):.2f}\")\n",
    "print(f\"Recall:    {recall(tp_without_context, fn_without_context):.2f}\")\n",
    "print(f\"Accuracy:  {accuracy(tp_without_context, fp_without_context, fn_without_context):.2f}\")\n",
    "\n",
    "print(\"\\n== With Context ==\")\n",
    "print(f\"Precision: {precision(tp_with_context, fp_with_context):.2f}\")\n",
    "print(f\"Recall:    {recall(tp_with_context, fn_with_context):.2f}\")\n",
    "print(f\"Accuracy:  {accuracy(tp_with_context, fp_with_context, fn_with_context):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ba32cd1c4443d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe14ca08897737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
