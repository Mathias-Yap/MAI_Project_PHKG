import os
import sys
import inspect

currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parentdir = os.path.dirname(currentdir)
sys.path.insert(0, parentdir) 

import json
import re
from typing import Dict, List, Optional, Union, Tuple
from LLM_Pipeline import ModelPipeline
from query_types import ChainQuery, StarQuery, OtherQuery 
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
class LLMQueryClassifier:
    """
    An LLM-based zero-shot classifier for identifying chain queries, star queries, or other types.
    
    Chain Query: A query that asks for a path or connection between two entities
    Star Query: A query that asks for multiple related items around a central concept
    """
    def __init__(self, model_name: str = "meta-llama/Llama-3.2-1B"):
        """
        Initializes the classifier with a specified model.
        
        :param model_name: The name of the LLM model to use for classification.
        """
        self.model_pipeline = ModelPipeline(model_name)
        self.system_prompt = {
            """You are a system tasked with classifying natural language queries into three categories, based on the structure they would follow in a personal health knowledge graph context.
            You ONLY return one of the FOLLOWING labels based on the query:
            "chain"
            "star"
            "other" 
            """
        }
        self.prompt_template = (
                "Classify a natural language query into one of the following three categories "
                "based on its structure and intent:\n\n"
                "1. CHAIN\n"
                "- A Chain query asks for a path or relationship between two specific entity types. "
                "It typically follows a linear connection, potentially passing and retrieving intermediate entities, "
                "but the goal is to find a certain type starting at another entity.\n"
                "- Examples: {chain_examples}\n\n"
                "2. STAR\n"
                "- A Star query asks for multiple facts or entities related to a single central concept. "
                "Typically, it involves retrieving multiple items that are connected to a central entity. "
                "- Examples: {star_examples}\n\n"
                "3. OTHER\n"
                "- Any query that does not fit the above two patterns (e.g., vague, meta-queries, or structurally unrelated questions).\n\n"
                "Your task:\n"
                "Note: Return ONLY one of the following labels based on the query:\n"
                "Chain\n"
                "Star\n"
                "Other\n"
                "If the query is a chain, find the start and end entities, and if applicable, the type of connection to retrieve.\n"
                "If the query is a star, identify the central entity and the related items (arms) to retrieve.\n"
                "If the query does not fit either pattern, classify it as 'other'.\n\n"
                "Answer in JSON format with the following structure:\n"
                "{{"
                "  \"type\": \"chain\" | \"star\" | \"other\",\n"
                "  \"start\": \"<start entity>\" (if applicable),\n"
                "  \"end\": \"<end entity>\" (if applicable),\n"
                "  \"retrieve\": \"<type of connection to retrieve>\" (if applicable),\n"
                "  \"center\": \"<central entity>\" (if applicable),\n"
                "  \"arms\": [\"<related item 1>\", \"<related item 2>\", ...] (if applicable)\n"
                "}}\n"
                "Return no other text or explanations.\n\n"
            )
            # "You are a query classifier. Classify queries into three types:\n\n"
            # "1. CHAIN QUERY: Asks for a connection, path, or relationship between two specific entities\n"
            # "   - Examples: {chain_example}\n\n"
            # "2. STAR QUERY: Asks for multiple related items around a central concept\n"
            # "   - Examples: {star_example}\n\n"
            # "3. OTHER: Anything that doesn't fit the above patterns\n\n"
            # "Format your response as a single one of these three words:\n"
            # '- Chain \n '
            # '- Star \n'
            # '- Other \n'

        self.examples = []
    
    def _create_prompt(self, query: str) -> str:
        # Concatenate all examples of the same type for the prompt

        # Build context for chain queries
        chain_contexts = []
        for ex in self.examples:
            if ex.get('type') == 'chain':
                parts = []
                if ex.get('start'):
                    parts.append(f"start: {ex['start']}")
                if ex.get('end'):
                    parts.append(f"end: {ex['end']}")
                if ex.get('retrieve'):
                    parts.append(f"retrieve: {ex['retrieve']}")
                if parts:
                    chain_contexts.append(" | ".join(parts))

        # Build context for star queries
        star_contexts = []
        for ex in self.examples:
            if ex.get('type') == 'star':
                parts = []
                if ex.get('center'):
                    parts.append(f"center: {ex['center']}")
                if ex.get('arms'):
                    parts.append(f"arms: {', '.join(ex['arms'])}")
                if ex.get('retrieve'):
                    parts.append(f"retrieve: {ex['retrieve']}")
                if parts:
                    star_contexts.append(" | ".join(parts))

        star_examples = ""
        chain_examples =  ""
        for ex, context in zip(self.examples, star_contexts):
            if ex.get('type') == 'star':
                star_examples += f"Example: {ex['query']} | Context: {context}\n"
            if ex.get('type') == 'chain':
                chain_examples += f"Example: {ex['query']} | Context: {context}\n"

        return self.prompt_template.format(
            chain_examples=chain_examples,
            star_examples=star_examples
        ) + f"\nQuery: {query}\nResponse:"
        
        
    def classify(self, query: str, max_new_tokens: int = 150): 
        """
        Classify a query using the LLM.
        
        Args:
            query: The input query to classify
            max_new_tokens: Maximum new tokens to generate
            
        Returns:
            Classification result
        """
        if not query or not query.strip():
            raise ValueError("Query must be a non-empty string.")
        
        prompt = self._create_prompt(query)
        
        return self.model_pipeline.generate(prompt, self.system_prompt) 
        
        
        
    def classify_batch(self, queries: List[str], max_new_tokens: int = 150) -> List[Union[Dict[str,str], None]]:
        """
        Classify multiple queries in batch.
        
        Args:
            queries: List of queries to classify
            max_new_tokens: Maximum new tokens to generate per query
            
        Returns:
            List of classification results
        """
        results = []
        for query in queries:
            result = self.classify(query, max_new_tokens)
            results.append(result)
        return results

    # def _parse_llm_response(self, response: str) -> Union[ChainQuery, StarQuery, OtherQuery]:
    #     """
    #     Parse the LLM response into a classification object.
        
    #     Args:
    #         response: Raw response from LLM
            
    #     Returns:
    #         Classification object
    #     """
    #     try:
    #         # Try to extract JSON from the response
    #         json_match = re.search(r'\{.*\}', response, re.DOTALL)
    #         if json_match:
    #             json_str = json_match.group()
    #             result = json.loads(json_str)
                
    #             if result.get("type") == "chain":
    #                 return ChainQuery(
    #                     start=result.get("start", ""),
    #                     end=result.get("end", ""),
    #                     retrieve=result.get("retrieve", "connection")
    #                 )
    #             elif result.get("type") == "star":
    #                 return StarQuery(
    #                     center=result.get("center", ""),
    #                     arms=result.get("arms", [])
    #                 )
    #             else:
    #                 return OtherQuery()
            
    #         # Fallback parsing if JSON extraction fails
    #         response_lower = response.lower()
    #         if any(word in response_lower for word in ["chain", "connection", "between", "from", "to"]):
    #             # Try to extract components from text
    #             start_match = re.search(r'start["\s:]*([^",\n]+)', response_lower)
    #             end_match = re.search(r'end["\s:]*([^",\n]+)', response_lower)
    #             retrieve_match = re.search(r'retrieve["\s:]*([^",\n]+)', response_lower)
                
    #             return ChainQuery(
    #                 start=start_match.group(1).strip() if start_match else "",
    #                 end=end_match.group(1).strip() if end_match else "",
    #                 retrieve=retrieve_match.group(1).strip() if retrieve_match else "connection"
    #             )
            
    #         elif any(word in response_lower for word in ["star", "center", "arms", "around"]):
    #             center_match = re.search(r'center["\s:]*([^",\n]+)', response_lower)
    #             arms_match = re.search(r'arms["\s:]*\[([^\]]+)\]', response_lower)
                
    #             arms = []
    #             if arms_match:
    #                 arms_str = arms_match.group(1)
    #                 arms = [arm.strip().strip('"\'') for arm in arms_str.split(',')]
                
    #             return StarQuery(
    #                 center=center_match.group(1).strip() if center_match else "",
    #                 arms=arms
    #             )
            
    #         return OtherQuery()
            
    #     except Exception as e:
    #         print(f"Error parsing LLM response: {e}")
    #         print(f"Response was: {response}")
    #         return OtherQuery()

if __name__ == "__main__":
    # Example usage
    classifier = LLMQueryClassifier(model_name="meta-llama/Llama-3.2-1B-Instruct")
    examples = [
        {"query": "What Pseudo-identifiers are related to this Lab Test Event?", "type": "chain", "start": "Lab Test Event", "end": "Pseudo-identifiers", "retrieve": "Pseudo-identifiers"},
        {"query": "What active substances has patient 49150 been prescribed?", "type": "chain", "start": "patient 49150", "end": "active subtstances", "retrieve": "active substances"},
        {"query": "What medical procedures and diagnoses did patient 102385 have?", "type": "star", "center": "patient 102385", "arms": ["medical procedures", "diagnoses"], "retrieve": "medical procedures and diagnoses"},
        {"query": "What Coding System, Code and Lab Test event are related to this diagnosis?", "type": "star", "center": "diagnosis", "arms": ["Coding System", "Code", "Lab Test Event"], "retrieve": "Coding System, Code and Lab Test Event"},
    ]
    classifier.examples = examples
    # print(classifier.prompt_template.format(
    #     chain_example=examples[0]['query'],
    #     star_example=examples[1]['query']
    # ))
    print(classifier._create_prompt("What substances and diagnoses are related to this Lab Test Event?"))
    print(classifier.classify("What substances are related to this Lab Test Event?"))