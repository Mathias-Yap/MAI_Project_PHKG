{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-11T12:25:20.088268Z",
     "start_time": "2025-06-11T12:25:20.078765Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# Step 1: Parse the list string into an actual list if needed\n",
    "def fix_ranked_list(value):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except Exception:\n",
    "            return []  # fallback if parsing fails\n",
    "    return value  # already a list\n",
    "\n",
    "\n",
    "# Step 2: Normalize templates\n",
    "def normalize_template(template):\n",
    "    if not isinstance(template, str):\n",
    "        return \"\"\n",
    "    template = template.replace('\\r\\n', '\\n').strip()\n",
    "    return re.sub(r'\\s+', ' ', template)\n",
    "\n",
    "\n",
    "# Step 3: Find rank\n",
    "def find_template_rank(row):\n",
    "    ranked_list_raw = fix_ranked_list(row['all_prompts_ranked'])\n",
    "    ranked_list = [normalize_template(t) for t in ranked_list_raw]\n",
    "    template = normalize_template(row['sparql_template'])\n",
    "    try:\n",
    "        return ranked_list.index(template) + 1\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def clean_and_parse_result(raw):\n",
    "    if isinstance(raw, list):\n",
    "        return raw\n",
    "    if not isinstance(raw, str):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Replace single quotes with double quotes\n",
    "        raw = raw.replace(\"'\", '\"')\n",
    "\n",
    "        # Regex: match any value after a colon that is NOT already quoted, and quote it\n",
    "        raw = re.sub(r'(\": )([^\"{\\[\\],\\s][^,\\]}]*)', r'\\1\"\\2\"', raw)\n",
    "\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_iris(results):\n",
    "    \"\"\"\n",
    "    Extract patient IRIs as strings from a list of dicts like [{'patient': IRI<...>}]\n",
    "    Can also be used for different results like mean or count should not matter\n",
    "    \"\"\"\n",
    "    return set(str(list(d.values())[0]) for d in results)\n",
    "\n",
    "\n",
    "def compute_precision_recall(row):\n",
    "    result_list = clean_and_parse_result(row['result'])\n",
    "    expected_list = clean_and_parse_result(row['expected_result'])\n",
    "\n",
    "    result_set = extract_iris(result_list)\n",
    "    expected_set = extract_iris(expected_list)\n",
    "\n",
    "    true_positives = result_set & expected_set\n",
    "    precision = len(true_positives) / len(result_set) if result_set else 0\n",
    "    recall = len(true_positives) / len(expected_set) if expected_set else 0\n",
    "    if len(expected_set) == 0:\n",
    "        if len(result_set) == 0:\n",
    "            precision = 1\n",
    "            recall = 1\n",
    "\n",
    "    exact = result_set == expected_set\n",
    "\n",
    "    return pd.Series([precision, recall, exact])\n",
    "\n",
    "\n",
    "def load_result_data(test_set_path, result_path):\n",
    "    #Load in the data\n",
    "    test_set_df = pd.read_csv(test_set_path)\n",
    "    pipeline_results_df = pd.read_csv(result_path)\n",
    "    # Rename columns for merging\n",
    "    pipeline_results_df.rename(columns={'natural_language_question': 'question'}, inplace=True)\n",
    "    # Make sure to remove erronous testdata\n",
    "    test_set_df = test_set_df[test_set_df['question'].map(test_set_df['question'].value_counts()) == 1]\n",
    "    pipeline_results_df = pipeline_results_df[\n",
    "        pipeline_results_df['question'].map(pipeline_results_df['question'].value_counts()) == 1]\n",
    "    # Merge the data\n",
    "    merged_df = pd.merge(pipeline_results_df, test_set_df, on=\"question\", how='inner')\n",
    "    # Remove more errournous data\n",
    "    merged_df = merged_df[~merged_df['question'].str.contains(r'{', na=False)]\n",
    "\n",
    "    # Get template rank from the retrieved vector store\n",
    "    merged_df['template_rank'] = merged_df.apply(find_template_rank, axis=1)\n",
    "\n",
    "    # Get Precision Recal and Accuracy Metrics\n",
    "    merged_df[['precision', 'recall', 'exact_match']] = merged_df.apply(compute_precision_recall, axis=1)\n",
    "\n",
    "    # Prepare a boiled down summary df\n",
    "    columns_to_keep = [\"validation_time\", \"attempts\", \"initial_query_time\", \"final_query_execution_time\", \"total_time\",\n",
    "                       \"template_rank\",\n",
    "                       'precision', 'recall', 'exact_match', \"sparql_template\"]\n",
    "\n",
    "    reduced_merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "    # 1) Work on an explicit copy to avoid SettingWithCopyWarning\n",
    "    reduced = reduced_merged_df.copy()\n",
    "\n",
    "    # Safely parse and sum lists from string representations\n",
    "    reduced[\"validation_time\"] = reduced_merged_df[\"validation_time\"].apply(\n",
    "        lambda x: sum(ast.literal_eval(x)) if isinstance(x, str) else sum(x) if isinstance(x, list) else 0\n",
    "    )\n",
    "\n",
    "    # 2) Add exact_match_int\n",
    "    reduced['exact_match_int'] = reduced['exact_match'].astype(int)\n",
    "\n",
    "    # 3) Group and compute means on numeric columns only\n",
    "    #    Pandas >= 1.5 lets you pass numeric_only=True to .mean()\n",
    "    numeric_means = (\n",
    "        reduced\n",
    "        .groupby('sparql_template', as_index=False)\n",
    "        .mean(numeric_only=True)\n",
    "    )\n",
    "\n",
    "    # 4) Convert the fraction into a percentage\n",
    "    numeric_means['exact_match_percent'] = numeric_means['exact_match_int'] * 100\n",
    "\n",
    "    # 5) (Optional) bring back the count per template\n",
    "    counts = reduced.groupby('sparql_template').size().rename('n_queries').reset_index()\n",
    "    summary = numeric_means.merge(counts, on='sparql_template')\n",
    "\n",
    "    # 6) (Optional) drop the raw exact_match_int column\n",
    "    summary = summary.drop(columns=['exact_match_int'])\n",
    "\n",
    "    return merged_df, summary\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T12:26:11.909719Z",
     "start_time": "2025-06-11T12:26:11.814761Z"
    }
   },
   "cell_type": "code",
   "source": "merged_df, summary_df = load_result_data('../test_set.csv', '../pipeline_results.csv')",
   "id": "13e253ca99ed00ae",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed051b6aee503194"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
